{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\"\"\"\nimport pandas as pd\n\n# Load the CSV file into a pandas dataframe\ndf = pd.read_csv(\"/kaggle/input/aisbrest/ais_brest_locations.csv\")\n\n# Fetch all rows where the ship ID is 227362110\nship_data = df[df['id'] == 227362110]\n\n# Print the resulting dataframe\nprint(ship_data)\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2023-04-23T13:20:54.216537Z","iopub.execute_input":"2023-04-23T13:20:54.216972Z","iopub.status.idle":"2023-04-23T13:20:55.683083Z","shell.execute_reply.started":"2023-04-23T13:20:54.216933Z","shell.execute_reply":"2023-04-23T13:20:55.681698Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Importing ship data","metadata":{}},{"cell_type":"code","source":"\"\"\"\nimport pandas as pd\nimport numpy as np\n\n# Load the CSV file into a pandas dataframe\ndf = pd.read_csv(\"/kaggle/input/aisbrest/ais_brest_locations.csv\")\n\n# Fetch all rows where the ship ID is 227362110\n#ship_data = df[df['id'] == 227362110] #2008\n#ship_data = df[df['id'] == 245334000] #10.000\nship_data = df[df['id'] ==227635210] #22347 --default\n#ship_data = df[df['id'] ==227306100] #24320\n#ship_data = df[df['id'] ==227574020] #54367\n\n\n# Select only the desired columns and sort by timestamp\ndesired_columns = ['timestamp','longitude', 'latitude']\nship_data = ship_data[desired_columns].sort_values(by='timestamp')\n\nprint(ship_data)\n\n#ship_data_matrix = ship_data.to_numpy()\n\n# Set the display format of NumPy arrays\n#np.set_printoptions(formatter={'float': lambda x: \"{:.8f}\".format(x)})\n\n# Print the resulting matrix\n#print(ship_data_matrix)\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2023-05-21T12:13:40.901504Z","iopub.execute_input":"2023-05-21T12:13:40.901945Z","iopub.status.idle":"2023-05-21T12:13:43.510897Z","shell.execute_reply.started":"2023-05-21T12:13:40.901907Z","shell.execute_reply":"2023-05-21T12:13:43.509552Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Load the CSV file into a pandas dataframe\ndf = pd.read_csv(\"/kaggle/input/aisbrest/ais_brest_locations.csv\")\n\nship_data = df[df['id'] ==227635210] #22347 --default\n\nship_data = ship_data[ship_data['latitude'] <= 50]\nship_data = ship_data[ship_data['longitude'] >= -5]\n# Select only the desired columns and sort by timestamp\nship_data = ship_data.sort_values(by='timestamp')\n\n\n# Select only the desired columns and sort by timestamp\n#desired_columns = ['timestamp','longitude', 'latitude']\n#ship_data = ship_data[desired_columns].sort_values(by='timestamp')\n\nprint(ship_data)\n\nfig, ax = plt.subplots(figsize=(15, 6))\n# Create scatterplot between longitude and speed\nplt.scatter(ship_data['longitude'], ship_data['speed'])\nplt.xlabel('Speed')\nplt.ylabel('Longitude')\nplt.title('Scatterplot of Longitude and Speed')\nplt.show()\n\nfig, ax = plt.subplots(figsize=(15, 6))\n# Create scatterplot between latitude and speed\nplt.scatter(ship_data['latitude'], ship_data['speed'])\nplt.xlabel('Speed')\nplt.ylabel('Latitude')\nplt.title('Scatterplot of Latitude and Speed')\nplt.show()\n\n# Select only the desired columns and the first 1000 rows\n#ship_data_1000 = ship_data[['longitude', 'speed']][:1000]\n\n# Create the scatterplot\n#plt.scatter(ship_data_1000['longitude'], ship_data_1000['speed'])\n#plt.xlabel('Speed')\n#plt.ylabel('Longitude')\n#plt.show()\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2023-05-07T19:34:20.525991Z","iopub.execute_input":"2023-05-07T19:34:20.526373Z","iopub.status.idle":"2023-05-07T19:34:21.826148Z","shell.execute_reply.started":"2023-05-07T19:34:20.526339Z","shell.execute_reply":"2023-05-07T19:34:21.824757Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data evaluation","metadata":{}},{"cell_type":"code","source":"\"\"\"\nimport numpy as np\n\nship_data = df[df['id'] ==227635210] #22347\n#ship_data = df[df['id'] == 245334000] #10.000\n\nship_data = ship_data[ship_data['latitude'] <= 50]\nship_data = ship_data[ship_data['longitude'] >= -5]\n# Select only the desired columns and sort by timestamp\ndesired_columns = ['timestamp','longitude', 'latitude']\nship_data = ship_data[desired_columns].sort_values(by='timestamp')\n\nprint(ship_data)\n#22285 \n\n# Convert longitude and latitude columns to numpy arrays\nlongitude = ship_data['longitude'].values\nlatitude = ship_data['latitude'].values\n\n#print(longitude)\n#print(latitude)\n\n# Calculate Pearson's correlation coefficient\npearsons_coefficient = np.corrcoef(ship_data['longitude'], ship_data['latitude'])\n\nprint(\"\\nThe Pearson's correlation coefficient of the longitude and latitude columns is:\\n\", pearsons_coefficient)\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2023-05-21T12:13:47.999522Z","iopub.execute_input":"2023-05-21T12:13:47.999971Z","iopub.status.idle":"2023-05-21T12:13:48.028932Z","shell.execute_reply.started":"2023-05-21T12:13:47.999923Z","shell.execute_reply":"2023-05-21T12:13:48.027592Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#from decimal import Decimal\n\n#float_array = ship_data.astype(float)\n#print(float_array)","metadata":{"execution":{"iopub.status.busy":"2023-04-23T18:55:49.933726Z","iopub.execute_input":"2023-04-23T18:55:49.934394Z","iopub.status.idle":"2023-04-23T18:55:49.950225Z","shell.execute_reply.started":"2023-04-23T18:55:49.934339Z","shell.execute_reply":"2023-04-23T18:55:49.948044Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# test 1","metadata":{}},{"cell_type":"code","source":"\"\"\"\nimport numpy as np\nfrom numpy import asarray\nfrom pandas import DataFrame\nfrom pandas import concat\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_absolute_percentage_error\nfrom sklearn.metrics import median_absolute_error\nfrom sklearn.metrics import max_error\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import explained_variance_score\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import AdaBoostRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom matplotlib import pyplot\nfrom xgboost import XGBRegressor\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.layers import LSTM\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import MinMaxScaler\n#from statsmodels.tsa.arima.model import ARIMA\nfrom statsmodels.tsa.vector_ar.var_model import VAR\nfrom prophet import Prophet\nfrom sklearn.svm import SVR\n#import statsmodels.api as sm\n\n\n# Define the models\nmodels = {\n    #lstm': lambda input_shape: Sequential([ #different coding\n    #    LSTM(units=64, input_shape=(1, look_back)),\n    #    Dense(units=1)]),\n    'xgb': XGBRegressor(objective='reg:squarederror', n_estimators=50), \n    'rf': RandomForestRegressor(n_estimators=100),\n    #'var': VAR, #different coding\n    'dt': DecisionTreeRegressor(max_depth = 50),\n    #'prophet': Prophet, #different coding\n    #'svr': SVR(kernel='rbf', gamma=0.5, C=10, epsilon = 0.05),\n    'etr':ExtraTreesRegressor(n_estimators=50),\n    #'ada':AdaBoostRegressor(n_estimators=50)\n    'knr':KNeighborsRegressor(n_neighbors=50)\n}\n\n# model= models['xgb']\n\n\n\n# transform a time series dataset into a supervised learning dataset\n#creating a matrix where each row represens a time step and each column represents a feauture.\n#n_in (number of time steps to use as input for the model)\n#n_out (number of time steps to forecast)\n\n\ndef series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n\tn_vars = 1 if type(data) is list else data.shape[1] # 1 or #of input columns\n\t#df = DataFrame(data)\n\tdf = DataFrame(data[:,1:]) #exclude the first column which is the time index.\n\ttimestamp=DataFrame(data[:,1]) #dataframe of the time index\n\tcols = list() #empty list to hold the lagged and forecasted data.\n\t# input sequence (t-n, ... t-1)\n\tfor i in range(n_in, 0, -1):\n\t\tcols.append(df.shift(i)) # lagged input sequence.\n\t# forecast sequence (t, t+1, ... t+n)\n\tfor i in range(0, n_out):\n\t\tcols.append(df.shift(-i)) \n\t# put it all together\n\tagg = concat(cols, axis=1)\n\t# drop rows with NaN values\n\tif dropnan:\n\t\tagg.dropna(inplace=True)\n\treturn agg.values\n\n\n\n# load the dataset\n#data = ship_data.values\n#data = [[1, 10, 10], [2, 20, 20], [3, 30, 30], [4, 40, 40], [5, 50, 50], [6, 60, 60], [7, 70, 70], [8, 80, 80], [9, 90, 90], [10, 100, 100]]\n#ship_data = np.array(data)\n#ship_data = pd.DataFrame(data)\n#print(ship_data)\n#print(ship_data.values)\n#result = series_to_supervised(ship_data,1,2) \n#print(result)\n\n\n\n\n# split a univariate dataset into train/test sets\n#The input parameters are data (the dataset to split) and n_test (the number of time steps to use for testing)\ndef train_test_split(data, n_test):\n\treturn data[:-n_test, :], data[-n_test:, :]\n\n#The first numpy array returned, represents the training dataset, \n#which consists of all the observations in the original data array except for the last n_test observations.\n#The second numpy array returned, represents the test dataset,\n#which consists of the last n_test observations of the original data array.\n\n# fit an random forest model and make a one step prediction\ndef forecast(model_name, train, testX, feautures, n_out):\n\t# transform list into array\n\ttrain = asarray(train)\n\t# split into input and output columns\n\ttrainX, trainy = train[:, :-1*feautures*n_out], train[:,-1*feautures*n_out:]\n\t#print('random_forest_shapes',trainX.shape,trainy.shape,trainy[0],train.shape)   \n\tmodel= models[model_name]\n\tmodel.fit(trainX, trainy)\n\t# make a one-step prediction\n\tyhat = model.predict([testX])\n\t#print(yhat.shape,yhat)\n\treturn yhat[0]\n\n#train, test = train_test_split(data, n_test)\n\n#result=forecast(model_name='rf', train=, testX,feautures=1, n_out=1)\n#print(yhat)\n\n\n\n# walk-forward validation for univariate data\ndef walk_forward_validation(model_name, data, feautures, n_out, n_test):\n\t#print(data.shape,data[0])\n\tpredictions = list()\n\t# split dataset\n\ttrain, test = train_test_split(data, n_test)\n\t#print('shape',train.shape,test.shape)   \n\t# seed history with training dataset\n\thistory = [x for x in train]\n\t#print(history[0])\n\t# step over each time-step in the test set\n\tfor i in range(len(test)):\n\t\t# split test row into input and output columns\n\t\ttestX, testy = test[i, :-1*feautures*n_out], test[i, -1*feautures*n_out:]\n\t\t# fit model on history and make a prediction\n\t\tyhat = forecast(model_name, history, testX, feautures, n_out)\n\t\t# store forecast in list of predictions\n\t\tpredictions.append(yhat)\n\t\t# add actual observation to history for the next loop\n\t\thistory.append(test[i])\n\t\toutput = \"(expected,predicted):\"\n\t\tfor j in range(feautures*n_out):\n\t\t\toutput = output  + \"(\" + '{0:.7f}'.format(test[i][j]) +\",\" + '{0:.7f}'.format(yhat[j]) + \")\"\n\t\tprint(output)\n\t\t#print(\"yhat\",i, \":\",yhat)       \n\t# estimate prediction error\n\tp=np.array(predictions)\n\t# Mean Absolute Error\n\tmae = [mean_absolute_error(test[:,-1*feautures*n_out+j],p[:,j]) for j in range(feautures*n_out)]\n\t# Mean Squared Error\n\tmse = [mean_squared_error(test[:,-1*feautures*n_out+j],p[:,j]) for j in range(feautures*n_out)]\n\t# MAPE- Mean absolute percentage error\n\tmape = [mean_absolute_percentage_error(test[:,-1*feautures*n_out+j],p[:,j]) for j in range(feautures*n_out)]    \n\t# Median absolute error\n\tMedAE = [median_absolute_error(test[:,-1*feautures*n_out+j],p[:,j]) for j in range(feautures*n_out)]\n\t# Max error\n\tMaxE = [max_error(test[:,-1*feautures*n_out+j],p[:,j]) for j in range(feautures*n_out)]\n\t# R² score, the coefficient of determination\n\tr2 = [r2_score(test[:,-1*feautures*n_out+j],p[:,j]) for j in range(feautures*n_out)]\n\t# Explained variance score\n\tevs = [explained_variance_score(test[:,-1*feautures*n_out+j],p[:,j]) for j in range(feautures*n_out)] \n\t# Information Criteria for Model Selection\n\tAIC =n_test* np.log(mse) + 2 * 1\n\tSBIC = n_test * np.log(mse) + np.log(n_test) * 1\n\tHQIC = n_test * np.log(mse) + 2 * np.log(np.log(n_test)) * 1\n\t#error = [mean_absolute_error(test[:, -3], p[:,2]),mean_absolute_error(test[:, -2], p[:,0]),mean_absolute_error(test[:, -1], p[:,1])]\n\treturn [mae,mse,mape,MedAE,MaxE,r2,evs,AIC,SBIC,HQIC], test[:,-1*feautures*n_out:], predictions\n\ndef predict(model_name, values, n_in, n_out, n_test):\n\t# load the dataset\n\tvalues = ship_data.values\n\t#values = float_array.values # VAR --else raws.values\n\t#values =np.hstack(values, input_column)\n\t# transform the time series data into supervised learning\n\tdata = series_to_supervised(values, n_in, n_out)    \n\tfeautures =  values.shape[1]-1\n\t#model = sm.tsa.vector_ar.var_model.VAR(endog=data)\n\t#model.set_default_exog_names(0) # VAR set the exog_names attribute to zero\n\t# evaluate\n\treturn walk_forward_validation(model_name, data, feautures, n_out, n_test)\n\n\n# HOMEMADE DEF\n\nimport matplotlib.pyplot as plt\nfrom matplotlib.gridspec import GridSpec\n\ndef print_everything(model_name, values, n_in, n_out, n_test):\n    print(\"\\nThe name of the model is:\", model_name)\n    model= models[model_name]\n    print(\"\\nExpected vs Predicted:\")\n    error , y , yhat  = predict(model_name, values, n_in, n_out, n_test)\n    \n    print(\"\\nThe error values for\", model_name,\":\")\n    MAE = \", \".join([\"%.10f\" % member for member in error[0]])\n    MSE = \", \".join([\"%.10f\" % member for member in error[1]])\n    mape = \", \".join([\"%.10f\" % member for member in error[2]])\n    MedAE = \", \".join([\"%.10f\" % member for member in error[3]])\n    MaxE = \", \".join([\"%.10f\" % member for member in error[4]])\n    r2 = \", \".join([\"%.5f\" % member for member in error[5]]) # it doesnt really apply in this\n    evs = \", \".join([\"%.5f\" % member for member in error[6]])\n    AIC =\", \".join([\"%.5f\" % member for member in error[7]])\n    SBIC =\", \".join([\"%.5f\" % member for member in error[8]])\n    HQIC =\", \".join([\"%.5f\" % member for member in error[9]])\n    #SBIC\n    #HQIC\n    print(f\"\\nMAE = {MAE} \\nMSE = {MSE} \\nmape = {mape} \\nMedAE = {MedAE} \\nMaxE = {MaxE} \\nr2 = {r2} \\nevs = {evs}\")\n    print(\"\\nThe Information Criteria for\", model_name,\":\")\n    print(f\"\\nAIC = {AIC}  \\nSBIC = {SBIC} \\nHQIC = {HQIC}\")\n    print(\"\\n\\n\")\n    \n    fig, axs = plt.subplots(2, 1, figsize=(10, 8))\n    plt.subplots_adjust(hspace=0.4)\n\n    axs[0].set_title(\"Expected vs Predicted Long\")\n    axs[0].set_xlabel(\"Sample\")\n    axs[0].set_ylabel(\"Long\")\n    axs[0].plot(np.array(y)[:, 0], label=\"Expected Long\")\n    axs[0].plot(np.array(yhat)[:, 0], label=\"Predicted Long\")\n    ymin = min(np.min(np.array(y)[:, 0]), np.min(np.array(yhat)[:, 0]))\n    ymax = max(np.max(np.array(y)[:, 0]), np.max(np.array(yhat)[:, 0]))\n    axs[0].set_ylim(ymin, ymax)\n    axs[0].legend()\n\n    axs[1].set_title(\"Expected vs Predicted Lat\")\n    axs[1].set_xlabel(\"Sample\")\n    axs[1].set_ylabel(\"Lat\")\n    axs[1].plot(np.array(y)[:, 1], label=\"Expected Lat\")\n    axs[1].plot(np.array(yhat)[:, 1], label=\"Predicted Lat\")\n    ymin = min(np.min(np.array(y)[:, 1]), np.min(np.array(yhat)[:, 1]))\n    ymax = max(np.max(np.array(y)[:, 1]), np.max(np.array(yhat)[:, 1]))\n    axs[1].set_ylim(ymin, ymax)\n    axs[1].legend()\n\n    plt.show()\n\"\"\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Code","metadata":{}},{"cell_type":"code","source":"\"\"\"\nimport numpy as np\nfrom numpy import asarray\nfrom pandas import DataFrame\nfrom pandas import concat\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_absolute_percentage_error\nfrom sklearn.metrics import median_absolute_error\nfrom sklearn.metrics import max_error\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import explained_variance_score\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import AdaBoostRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom matplotlib import pyplot\nfrom xgboost import XGBRegressor\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.layers import LSTM\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import MinMaxScaler\n#from statsmodels.tsa.arima.model import ARIMA\nfrom statsmodels.tsa.vector_ar.var_model import VAR\nfrom prophet import Prophet\nfrom sklearn.svm import SVR\n#import statsmodels.api as sm\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import ElasticNet\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.linear_model import HuberRegressor\n#from sklearn.ensemble import BaggingRegressor\nfrom sklearn.ensemble import AdaBoostRegressor\nfrom sklearn.linear_model import Ridge\n\n\n\n\n\n\n# Define the models\nmodels = {\n    #lstm': lambda input_shape: Sequential([ #different coding\n    #    LSTM(units=64, input_shape=(1, look_back)),\n    #    Dense(units=1)]),\n    'xgb': XGBRegressor(objective='reg:squarederror', n_estimators=50), \n    'rf': RandomForestRegressor(n_estimators=100),\n    #'var': VAR, #different coding\n    'dt': DecisionTreeRegressor(max_depth = 50),\n    #'prophet': Prophet, #different coding\n    #'svr': SVR(kernel='rbf', gamma=0.5, C=10, epsilon = 0.05),\n    'etr':ExtraTreesRegressor(n_estimators=50),\n    #'ada':AdaBoostRegressor(n_estimators=50)\n    'knr':KNeighborsRegressor(n_neighbors=50),\n    'Lasso':Lasso(max_iter=1000),\n    'ElasticNet':ElasticNet(max_iter=1000),\n    'krr':KernelRidge(alpha=1.0,kernel='polynomial',degree=4),\n    'Ridge':Ridge(alpha=0.3),\n    'Huber':HuberRegressor(max_iter=100),\n    #'Bagging':BaggingRegressor()\n    'Ada':AdaBoostRegressor(),\n    'svr_poly' : SVR(kernel=\"poly\", C=100, gamma=\"auto\", degree=3, epsilon=0.1, coef0=1)\n}\n\n# model= models['xgb']\n\n\n\n# transform a time series dataset into a supervised learning dataset\n#creating a matrix where each row represens a time step and each column represents a feauture.\n#n_in (number of time steps to use as input for the model)\n#n_out (number of time steps to forecast)\n\n\ndef series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n\tn_vars = 1 if type(data) is list else data.shape[1] # 1 or #of input columns\n\t#df = DataFrame(data)\n\tdf = DataFrame(data[:,1:]) #exclude the first column which is the time index.\n\ttimestamp=DataFrame(data[:,1]) #dataframe of the time index\n\tcols = list() #empty list to hold the lagged and forecasted data.\n\t# input sequence (t-n, ... t-1)\n\tfor i in range(n_in, 0, -1):\n\t\tcols.append(df.shift(i)) # lagged input sequence.\n\t# forecast sequence (t, t+1, ... t+n)\n\tfor i in range(0, n_out):\n\t\tcols.append(df.shift(-i)) \n\t# put it all together\n\tagg = concat(cols, axis=1)\n\t# drop rows with NaN values\n\tif dropnan:\n\t\tagg.dropna(inplace=True)\n\treturn agg.values\n\n\n\n# load the dataset\n#data = ship_data.values\n#data = [[1, 10, 10], [2, 20, 20], [3, 30, 30], [4, 40, 40], [5, 50, 50], [6, 60, 60], [7, 70, 70], [8, 80, 80], [9, 90, 90], [10, 100, 100]]\n#ship_data = np.array(data)\n#ship_data = pd.DataFrame(data)\n#print(ship_data)\n#print(ship_data.values)\n#result = series_to_supervised(ship_data,1,2) \n#print(result)\n\n\n\n\n# split a univariate dataset into train/test sets\n#The input parameters are data (the dataset to split) and n_test (the number of time steps to use for testing)\ndef train_test_split(data, n_test):\n\treturn data[:-n_test, :], data[-n_test:, :]\n\n#The first numpy array returned, represents the training dataset, \n#which consists of all the observations in the original data array except for the last n_test observations.\n#The second numpy array returned, represents the test dataset,\n#which consists of the last n_test observations of the original data array.\n\n# fit an random forest model and make a one step prediction\ndef forecast(model_name, train, testX, feautures, n_out):\n\t# transform list into array\n\ttrain = asarray(train)\n\t# split into input and output columns\n\ttrainX, trainy = train[:, :-1*feautures], train[:,-1*feautures:]\n\t#print('random_forest_shapes',trainX.shape,trainy.shape,trainy[0],train.shape)   \n\tmodel= models[model_name]\n\tmodel.fit(trainX, trainy)\n\t# make a one-step prediction\n\tyhat = model.predict([testX])\n\t#print(yhat.shape,yhat)\n\treturn yhat[0]\n\n#train, test = train_test_split(data, n_test)\n\n#result=forecast(model_name='rf', train=, testX,feautures=1, n_out=1)\n#print(yhat)\n\n\n\n# walk-forward validation for univariate data\ndef walk_forward_validation(model_name, data, feautures, n_out, n_test):\n\t#print(data.shape,data[0])\n\tpredictions = list()\n\t# split dataset\n\ttrain, test = train_test_split(data, n_test)\n\t#print('shape',train.shape,test.shape)   \n\t# seed history with training dataset\n\thistory = [x for x in train]\n\t#print(history[0])\n\t# step over each time-step in the test set\n\tfor i in range(len(test)):\n\t\t# split test row into input and output columns\n\t\ttestX, testy = test[i, :-1*feautures], test[i, -1*feautures:]\n\t\t# fit model on history and make a prediction\n\t\tyhat = forecast(model_name, history, testX, feautures, n_out)\n\t\t# store forecast in list of predictions\n\t\tpredictions.append(yhat)\n\t\t# add actual observation to history for the next loop\n\t\thistory.append(test[i])\n\t\toutput = \"(expected,predicted):\"\n\t\tfor j in range(feautures):\n\t\t\toutput = output  + \"(\" + '{0:.7f}'.format(test[i][j]) +\",\" + '{0:.7f}'.format(yhat[j]) + \")\"\n\t\tprint(output)\n\t\t#print(\"yhat\",i, \":\",yhat)       \n\t# estimate prediction error\n\tp=np.array(predictions)\n\t# Mean Absolute Error\n\tmae = [mean_absolute_error(test[:,-1*feautures+j],p[:,j]) for j in range(feautures)]\n\t# Mean Squared Error\n\tmse = [mean_squared_error(test[:,-1*feautures+j],p[:,j]) for j in range(feautures)]\n\t# MAPE- Mean absolute percentage error\n\tmape = [mean_absolute_percentage_error(test[:,-1*feautures+j],p[:,j]) for j in range(feautures)]    \n\t# Median absolute error\n\tMedAE = [median_absolute_error(test[:,-1*feautures+j],p[:,j]) for j in range(feautures)]\n\t# Max error\n\tMaxE = [max_error(test[:,-1*feautures+j],p[:,j]) for j in range(feautures)]\n\t# R² score, the coefficient of determination\n\tr2 = [r2_score(test[:,-1*feautures+j],p[:,j]) for j in range(feautures)]\n\t# Explained variance score\n\tevs = [explained_variance_score(test[:,-1*feautures+j],p[:,j]) for j in range(feautures)] \n\t# Information Criteria for Model Selection\n\tAIC =n_test* np.log(mse) + 2 * 1\n\tSBIC = n_test * np.log(mse) + np.log(n_test) * 1\n\tHQIC = n_test * np.log(mse) + 2 * np.log(np.log(n_test)) * 1\n\t#error = [mean_absolute_error(test[:, -3], p[:,2]),mean_absolute_error(test[:, -2], p[:,0]),mean_absolute_error(test[:, -1], p[:,1])]\n\treturn [mae,mse,mape,MedAE,MaxE,r2,evs,AIC,SBIC,HQIC], test[:,-1*feautures:], predictions\n\ndef predict(model_name, values, n_in, n_out, n_test):\n\t# load the dataset\n\tvalues = ship_data.values\n\t#values = float_array.values # VAR --else raws.values\n\t#values =np.hstack(values, input_column)\n\t# transform the time series data into supervised learning\n\tdata = series_to_supervised(values, n_in, n_out)    \n\tfeautures =  values.shape[1]-1\n\t#model = sm.tsa.vector_ar.var_model.VAR(endog=data)\n\t#model.set_default_exog_names(0) # VAR set the exog_names attribute to zero\n\t# evaluate\n\treturn walk_forward_validation(model_name, data, feautures, n_out, n_test)\n\n\n# HOMEMADE DEF\n\nimport matplotlib.pyplot as plt\nfrom matplotlib.gridspec import GridSpec\n\ndef print_everything(model_name, values, n_in, n_out, n_test):\n    print(\"\\nThe name of the model is:\", model_name)\n    model= models[model_name]\n    print(\"\\nExpected vs Predicted:\")\n    error , y , yhat  = predict(model_name, values, n_in, n_out, n_test)\n    \n    print(\"\\nThe error values for\", model_name,\":\")\n    MAE = \", \".join([\"%.10f\" % member for member in error[0]])\n    MSE = \", \".join([\"%.10f\" % member for member in error[1]])\n    mape = \", \".join([\"%.10f\" % member for member in error[2]])\n    MedAE = \", \".join([\"%.10f\" % member for member in error[3]])\n    MaxE = \", \".join([\"%.10f\" % member for member in error[4]])\n    r2 = \", \".join([\"%.5f\" % member for member in error[5]]) # it doesnt really apply in this\n    evs = \", \".join([\"%.5f\" % member for member in error[6]])\n    AIC =\", \".join([\"%.5f\" % member for member in error[7]])\n    SBIC =\", \".join([\"%.5f\" % member for member in error[8]])\n    HQIC =\", \".join([\"%.5f\" % member for member in error[9]])\n    #SBIC\n    #HQIC\n    print(f\"\\nMAE = {MAE} \\nMSE = {MSE} \\nmape = {mape} \\nMedAE = {MedAE} \\nMaxE = {MaxE} \\nr2 = {r2} \\nevs = {evs}\")\n    print(\"\\nThe Information Criteria for\", model_name,\":\")\n    print(f\"\\nAIC = {AIC}  \\nSBIC = {SBIC} \\nHQIC = {HQIC}\")\n    print(\"\\n\\n\")\n    \n    fig, axs = plt.subplots(2, 1, figsize=(10, 8))\n    plt.subplots_adjust(hspace=0.4)\n\n    axs[0].set_title(\"Expected vs Predicted Long\")\n    axs[0].set_xlabel(\"Sample\")\n    axs[0].set_ylabel(\"Long\")\n    axs[0].plot(np.array(y)[:, 0], label=\"Expected Long\")\n    axs[0].plot(np.array(yhat)[:, 0], label=\"Predicted Long\")\n    ymin = min(np.min(np.array(y)[:, 0]), np.min(np.array(yhat)[:, 0]))\n    ymax = max(np.max(np.array(y)[:, 0]), np.max(np.array(yhat)[:, 0]))\n    axs[0].set_ylim(ymin, ymax)\n    axs[0].legend()\n\n    axs[1].set_title(\"Expected vs Predicted Lat\")\n    axs[1].set_xlabel(\"Sample\")\n    axs[1].set_ylabel(\"Lat\")\n    axs[1].plot(np.array(y)[:, 1], label=\"Expected Lat\")\n    axs[1].plot(np.array(yhat)[:, 1], label=\"Predicted Lat\")\n    ymin = min(np.min(np.array(y)[:, 1]), np.min(np.array(yhat)[:, 1]))\n    ymax = max(np.max(np.array(y)[:, 1]), np.max(np.array(yhat)[:, 1]))\n    axs[1].set_ylim(ymin, ymax)\n    axs[1].legend()\n\n    plt.show()\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2023-05-21T12:18:37.960541Z","iopub.execute_input":"2023-05-21T12:18:37.960989Z","iopub.status.idle":"2023-05-21T12:18:38.028239Z","shell.execute_reply.started":"2023-05-21T12:18:37.960951Z","shell.execute_reply":"2023-05-21T12:18:38.026825Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\nprint(\"svr_poly, lag=1, n_out=1, test=1000\")\nprint_everything('svr_poly',ship_data, 1, 1, 10)\n#print(\"xgb - 5 - 1 - 1000\")\n#print_everything('xgb', ship_data, 5, 1, 1000)\n#print(\"dt - 5 - 1 - 1000\")\n#print_everything('dt', ship_data, 5, 1, 1000)\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2023-05-21T12:18:49.377152Z","iopub.execute_input":"2023-05-21T12:18:49.378308Z","iopub.status.idle":"2023-05-21T12:18:49.533716Z","shell.execute_reply.started":"2023-05-21T12:18:49.378257Z","shell.execute_reply":"2023-05-21T12:18:49.531813Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## LSTM","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.utils.data as data\n\ndf = pd.read_csv(\"/kaggle/input/aisbrest/ais_brest_locations.csv\")\n\nship_data = df[df['id'] ==227635210] #22347\n#ship_data = df[df['id'] == 245334000] #10.000\n\nship_data = ship_data[ship_data['latitude'] <= 50]\nship_data = ship_data[ship_data['longitude'] >= -5]\n# Select only the desired columns and sort by timestamp\n#desired_columns = ['timestamp','longitude', 'latitude']\nship_data = ship_data.sort_values(by='timestamp')\ndesired_columns = ['longitude']\nship_data = ship_data[desired_columns]\n\nprint(ship_data.values)\nplt.plot(ship_data.values)\n","metadata":{"execution":{"iopub.status.busy":"2023-05-21T14:43:51.515242Z","iopub.execute_input":"2023-05-21T14:43:51.515673Z","iopub.status.idle":"2023-05-21T14:43:53.483498Z","shell.execute_reply.started":"2023-05-21T14:43:51.515636Z","shell.execute_reply":"2023-05-21T14:43:53.482189Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.utils.data as data\n\ndf = pd.read_csv(\"/kaggle/input/aisbrest/ais_brest_locations.csv\")\n\nship_data = df[df['id'] ==227635210] #22347\n#ship_data = df[df['id'] == 245334000] #10.000\n\nship_data = ship_data[ship_data['latitude'] <= 50]\nship_data = ship_data[ship_data['longitude'] >= -5]\n# Select only the desired columns and sort by timestamp\n#desired_columns = ['timestamp','longitude', 'latitude']\nship_data = ship_data.sort_values(by='timestamp')\ndesired_columns = ['longitude']\nship_data = ship_data[desired_columns]\n\n#ship_data=timeseries\n\n\n# train-test split for time series\ntrain_size = int(len(ship_data) * 0.82052)\ntest_size = len(ship_data) - train_size\ntrain, test = ship_data[:train_size], ship_data[train_size:]\n\n\ndef create_dataset(dataset, lookback):\n    \"\"\"Transform a time series into a prediction dataset\n    \n    Args:\n        dataset: A numpy array of time series, first dimension is the time steps\n        lookback: Size of window for prediction\n    \"\"\"\n    X, y = [], []\n    for i in range(len(dataset)-lookback):\n        feature = dataset[i:i+lookback].astype(np.float64)\n        target = dataset[i+1:i+lookback+1].astype(np.float64)\n        X.append(feature)\n        y.append(target)\n    return torch.tensor(X), torch.tensor(y)\n\n\n\n\nlookback = 1\n#X_train, y_train = create_dataset(train, lookback=lookback)\n#X_test, y_test = create_dataset(test, lookback=lookback)\n\nX_train, y_train = create_dataset(train.values, lookback=lookback)\nX_test, y_test = create_dataset(test.values, lookback=lookback)\n\n\nclass AirModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.lstm = nn.LSTM(input_size=1, hidden_size=50, num_layers=1, batch_first=True)\n        self.linear = nn.Linear(50, 1)\n    def forward(self, x):\n        x = x.to(torch.float64)  # Convert the input tensor to float64\n        x, _ = self.lstm(x)\n        x = self.linear(x)\n        return x\n\n\nmodel = AirModel()\nmodel = model.double()  # Convert the model's parameters to double precision\noptimizer = optim.Adam(model.parameters())\nloss_fn = nn.MSELoss()\nloader = data.DataLoader(data.TensorDataset(X_train, y_train), shuffle=True, batch_size=8)\n\nn_epochs = 100\nfor epoch in range(n_epochs):\n    model.train()\n    for X_batch, y_batch in loader:\n        y_pred = model(X_batch)\n        loss = loss_fn(y_pred, y_batch)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n    # Validation\n    if epoch % 100 != 0:\n        continue\n    model.eval()\n    with torch.no_grad():\n        y_pred = model(X_train)\n        train_rmse = np.sqrt(loss_fn(y_pred, y_train))\n        y_pred = model(X_test)\n        test_rmse = np.sqrt(loss_fn(y_pred, y_test))\n    print(\"Epoch %d: train RMSE %.4f, test RMSE %.4f\" % (epoch, train_rmse, test_rmse))\n\nwith torch.no_grad():\n    # shift train predictions for plotting\n    train_plot = np.ones_like(ship_data) * np.nan\n    y_pred = model(X_train)\n    y_pred = y_pred[:, -1, :]\n    train_plot[lookback:train_size] = model(X_train)[:, -1, :]\n    # shift test predictions for plotting\n    test_plot = np.ones_like(ship_data) * np.nan\n    test_plot[train_size+lookback:len(ship_data)] = model(X_test)[:, -1, :]\n# plot\n#plt.plot(ship_data)\nplt.plot(ship_data.values)\nplt.plot(train_plot, c='r')\nplt.plot(test_plot, c='g')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-05-21T14:52:36.786317Z","iopub.execute_input":"2023-05-21T14:52:36.786805Z","iopub.status.idle":"2023-05-21T14:59:12.236672Z","shell.execute_reply.started":"2023-05-21T14:52:36.786763Z","shell.execute_reply":"2023-05-21T14:59:12.235264Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Regressors with different shape y","metadata":{}},{"cell_type":"markdown","source":"        \"\"\"\n    \n    from sklearn.linear_model import BayesianRidge\n    from sklearn.linear_model import SGDRegressor\n    from lightgbm import LGBMRegressor\n\n    \n    'Bayesian':BayesianRidge(n_iter=100),\n    'SGDRegressor':SGDRegressor(max_iter=1000),\n     'LGBM':LGBMRegressor()\n\n    \n    \"\"\"","metadata":{}},{"cell_type":"code","source":"\"\"\"\nimport matplotlib.pyplot as plt\nfrom matplotlib.gridspec import GridSpec\n\nfig = plt.figure(figsize=(10, 10))\n\n# define gridspec\ngs = GridSpec(nrows=3, ncols=2, height_ratios=[1, 1, 2])\ngs.update(wspace=0.4, hspace=0.4)\n\n# create the list of values\nmae = [\"%.10f\" % member for member in error[0]]\nMSE = [\"%.10f\" % member for member in error[1]]\nmape = [\"%.10f\" % member for member in error[2]]\nMedAE = [\"%.10f\" % member for member in error[3]]\nMaxE =  [\"%.10f\" % member for member in error[4]]\nr2 = [\"%.5f\" % member for member in error[5]]\nevs =[\"%.5f\" % member for member in error[6]]\n\n# convert the values to floats\nmae = [float(v) for v in mae]\nMSE = [float(v) for v in MSE]\nmape = [float(v) for v in mape]\nMedAE = [float(v) for v in MedAE]\nMaxE = [float(v) for v in MaxE]\nr2 = [float(v) for v in r2]\nevs = [float(v) for v in evs]\n\n# create the x-axis values (index positions)\nindices = list(range(len(mae)))\nindices = list(range(len(MSE)))\nindices = list(range(len(mape)))\nindices = list(range(len(MedAE)))\nindices = list(range(len(MaxE)))\nindices = list(range(len(r2)))\nindices = list(range(len(evs)))\n\nfig, axs = plt.subplots(nrows=2, ncols=4, figsize=(15, 6))\n\n# create the histogram\naxs[0,0].bar(indices, mae)\naxs[0,1].bar(indices, MSE)\naxs[0,2].bar(indices, mape)\naxs[0,3].bar(indices, MedAE)\naxs[1,0].bar(indices, MaxE)\naxs[1,1].bar(indices, r2)\naxs[1,2].bar(indices, evs)\n\n#axs[0].hist(mae, bins=10)\n#axs[1].hist(MSE, bins=10)\n#axs[2].hist(mape, bins=10)\n#axs[3].hist(MedAE, bins=10)\n#axs[4].hist(MaxE, bins=10)\n#axs[5].hist(r2, bins=10)\n#axs[6].hist(evs, bins=10)\n\n#axs[0,0] = fig.add_subplot(gs[0, 0])\naxs[0,0].set(xlabel='MAE Values', ylabel='Frequency', title='MAE')\n#axs[1] = fig.add_subplot(gs[0, 1])\naxs[0,1].set(xlabel='MSE Values', ylabel='Frequency', title='MSE')\n#axs[2] = fig.add_subplot(gs[1, 0])\naxs[0,2].set(xlabel='Mape Values', ylabel='Frequency', title='Mape')\n#axs[3] = fig.add_subplot(gs[1, 1])\naxs[0,3].set(xlabel='MedAE Values', ylabel='Frequency', title='MedAE')\n#axs[4] = fig.add_subplot(gs[2, :])\naxs[1,0].set(xlabel='MaxE Values', ylabel='Frequency', title='MaxE')\naxs[1,1].set(xlabel='R2 Values', ylabel='Frequency', title='R2')\naxs[1,2].set(xlabel='EVS Values', ylabel='Frequency', title='EVS')\n\nfig.subplots_adjust(left=0.06, bottom=0.1, right=0.95, top=0.95, wspace=0.6, hspace=0.5)\n\nplt.show()\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2023-05-02T18:44:44.665795Z","iopub.execute_input":"2023-05-02T18:44:44.666203Z","iopub.status.idle":"2023-05-02T18:44:44.735801Z","shell.execute_reply.started":"2023-05-02T18:44:44.666167Z","shell.execute_reply":"2023-05-02T18:44:44.733739Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# test 2","metadata":{}},{"cell_type":"code","source":"\"\"\"\nimport numpy as np\nfrom numpy import asarray\nfrom pandas import DataFrame\nfrom pandas import concat\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_absolute_percentage_error\nfrom sklearn.metrics import median_absolute_error\nfrom sklearn.metrics import max_error\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import explained_variance_score\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom matplotlib import pyplot\nfrom xgboost import XGBRegressor\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.layers import LSTM\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import MinMaxScaler\n#from statsmodels.tsa.arima.model import ARIMA\nfrom statsmodels.tsa.vector_ar.var_model import VAR\nfrom prophet import Prophet\nfrom sklearn.svm import SVR\n#import statsmodels.api as sm\n\n\n# Define the models\nmodels = {\n    'lstm': lambda input_shape: Sequential([ #different coding\n        LSTM(units=64, input_shape=(1, look_back)),\n        Dense(units=1)\n    ]),\n    'xgb': XGBRegressor(objective='reg:squarederror', n_estimators=25), \n    'rf': RandomForestRegressor(n_estimators=100),\n    'var': VAR, #different coding\n    'dt': DecisionTreeRegressor(max_depth = 20),\n    'prophet': Prophet, #different coding\n    'svr': SVR(kernel='rbf', gamma=0.5, C=10, epsilon = 0.05)\n}\n\n# model= models['xgb']\n\n\n\n# transform a time series dataset into a supervised learning dataset\n#creating a matrix where each row represens a time step and each column represents a feauture.\n#n_in (number of time steps to use as input for the model)\n#n_out (number of time steps to forecast)\n\n\ndef series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n\tn_vars = 1 if type(data) is list else data.shape[1] # 1 or #of input columns\n\tdf = DataFrame(data)\n\t#df = DataFrame(data[:,1:]) #exclude the first column which is the time index.\n\t#timestamp=DataFrame(data[:,1]) #dataframe of the time index\n\tcols = list() #empty list to hold the lagged and forecasted data.\n\t# input sequence (t-n, ... t-1)\n\tfor i in range(n_in, 0, -1):\n\t\tcols.append(df.shift(i)) # lagged input sequence.\n\t# forecast sequence (t, t+1, ... t+n)\n\tfor i in range(0, n_out):\n\t\tcols.append(df.shift(-i)) \n\t# put it all together\n\tagg = concat(cols, axis=1)\n\t# drop rows with NaN values\n\tif dropnan:\n\t\tagg.dropna(inplace=True)\n\treturn agg.values\n\n\n\n# load the dataset\n#data = ship_data.values\n#data = [[1, 10], [2, 20], [3, 30], [4, 40], [5, 50], [6, 60], [7, 70], [8, 80], [9, 90], [10, 100]]\n#data = np.array(data)\ndata = [x for x in range(10)]\nprint(data)\nresult = series_to_supervised(data,1,1) \nprint(result)\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2023-04-25T21:14:18.923607Z","iopub.execute_input":"2023-04-25T21:14:18.924344Z","iopub.status.idle":"2023-04-25T21:14:18.944528Z","shell.execute_reply.started":"2023-04-25T21:14:18.924290Z","shell.execute_reply":"2023-04-25T21:14:18.943518Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Time Series Forecasting - Diefferent Horizons","metadata":{}},{"cell_type":"code","source":"\"\"\"\nimport numpy as np\nfrom numpy import asarray\nfrom pandas import DataFrame\nfrom pandas import concat\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_absolute_percentage_error\nfrom sklearn.metrics import median_absolute_error\nfrom sklearn.metrics import max_error\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import explained_variance_score\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom matplotlib import pyplot\nfrom xgboost import XGBRegressor\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.layers import LSTM\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import MinMaxScaler\n#from statsmodels.tsa.arima.model import ARIMA\nfrom statsmodels.tsa.vector_ar.var_model import VAR\nfrom prophet import Prophet\nfrom sklearn.svm import SVR\n#import statsmodels.api as sm\n\n\n# Define the models\nmodels = {\n    'lstm': lambda input_shape: Sequential([ #different coding\n        LSTM(units=64, input_shape=(1, look_back)),\n        Dense(units=1)\n    ]),\n    'xgb': XGBRegressor(objective='reg:squarederror', n_estimators=25), \n    'rf': RandomForestRegressor(n_estimators=100),\n    'var': VAR #different coding\n    'dt': DecisionTreeRegressor(max_depth = 20),\n    'prophet': Prophet, #different coding\n    'svr': SVR(kernel='rbf', gamma=0.5, C=10, epsilon = 0.05)\n}\n\n# model= models['xgb']\n\n\n\n# transform a time series dataset into a supervised learning dataset\n#creating a matrix where each row represens a time step and each column represents a feauture.\n#n_in (number of time steps to use as input for the model)\n#n_out (number of time steps to forecast)\ndef series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n\tn_vars = 1 if type(data) is list else data.shape[1] # 1 or #of input columns\n\tdf = DataFrame(data[:,1:]) #exclude the first column which is the time index.\n\ttimestamp=DataFrame(data[:,1]) #dataframe of the time index\n\tcols = list() #empty list to hold the lagged and forecasted data.\n\t# input sequence (t-n, ... t-1)\n\tfor i in range(n_in, 0, -1):\n\t\tcols.append(df.shift(i)) # lagged input sequence.\n\t# forecast sequence (t, t+1, ... t+n)\n\tfor i in range(0, n_out):\n\t\tcols.append(df.shift(-i))\n\t# put it all together\n\tagg = concat(cols, axis=1)\n\t# drop rows with NaN values\n\tif dropnan:\n\t\tagg.dropna(inplace=True)\n\treturn agg.values\n\tprint(agg.values)\n\n\n\t# load the dataset\nvalues = ship_data.values\n\t#values = float_array.values # VAR --else raws.values\n\t#values =np.hstack(values, input_column)\n\t# transform the time series data into supervised learning\ndata = series_to_supervised(values, n_in, n_out)    \n\t#feautures =  values.shape[1]-1\n    \nprint(n_vars)\n\n# split a univariate dataset into train/test sets\n#The input parameters are data (the dataset to split) and n_test (the number of time steps to use for testing)\ndef train_test_split(data, n_test):\n\treturn data[:-n_test, :], data[-n_test:, :]\n\n#The first numpy array returned, represents the training dataset, \n#which consists of all the observations in the original data array except for the last n_test observations.\n#The second numpy array returned, represents the test dataset,\n#which consists of the last n_test observations of the original data array.\n\n# fit an random forest model and make a one step prediction\ndef random_forest_forecast(model_name, train, testX, feautures, n_out):\n\t# transform list into array\n\ttrain = asarray(train)\n\t# split into input and output columns\n\ttrainX, trainy = train[:, :-1*feautures*n_out], train[:,-1*feautures*n_out:]\n\t#print('random_forest_shapes',trainX.shape,trainy.shape,trainy[0],train.shape)\n\t# fit model\n\t#model = RandomForestRegressor(n_estimators=1000) #1000 trees -- Change it and see!\n\t#model=DecisionTreeRegressor(max_depth = 3)    \n\tmodel= models[model_name]\n\t# LSTM Parameters\n\t#model = Sequential()\n\t#look_back=1\n\t#model.add(LSTM(4, input_shape=(1, look_back)))\n\t#model.add(Dense(1))\n\t#model.compile(loss='mean_squared_error', optimizer='adam')\n\t#model.set_default_exog_names(0)\n\tmodel.fit(trainX, trainy)\n\t# make a one-step prediction\n\tyhat = model.predict([testX])\n\t#print(yhat.shape,yhat)\n\treturn yhat[0]\n\n\nimport matplotlib.pyplot as plt\nfrom matplotlib.gridspec import GridSpec\n\ndef print_everything(model_name, values, n_in, n_out, n_test):\n    print(\"\\nThe name of the model is:\", model_name)\n    model= models[model_name]\n    print(\"\\nExpected vs Predicted:\")\n    error , y , yhat  = predict(model_name, values, n_in, n_out, n_test)\n    \n    print(\"\\nThe error values for\", model_name)\n    MAE = \", \".join([\"%.10f\" % member for member in error[0]])\n    MSE = \", \".join([\"%.10f\" % member for member in error[1]])\n    mape = \", \".join([\"%.10f\" % member for member in error[2]])\n    MedAE = \", \".join([\"%.10f\" % member for member in error[3]])\n    MaxE = \", \".join([\"%.10f\" % member for member in error[4]])\n    r2 = \", \".join([\"%.5f\" % member for member in error[5]]) # it doesnt really apply in this\n    evs = \", \".join([\"%.5f\" % member for member in error[6]])\n    print(f\"MAE = {MAE} \\nMSE = {MSE} \\nmape = {mape} \\nMedAE = {MedAE} \\nMaxE = {MaxE} \\nr2 = {r2} \\nevs = {evs}\")\n    print(\"\\n\\n\")\n    \"\"\"\n ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Code","metadata":{}},{"cell_type":"code","source":"\"\"\"\nimport numpy as np\nfrom numpy import asarray\nfrom pandas import DataFrame\nfrom pandas import concat\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_absolute_percentage_error\nfrom sklearn.metrics import median_absolute_error\nfrom sklearn.metrics import max_error\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import explained_variance_score\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom matplotlib import pyplot\nfrom xgboost import XGBRegressor\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.layers import LSTM\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import MinMaxScaler\n#from statsmodels.tsa.arima.model import ARIMA\nfrom statsmodels.tsa.vector_ar.var_model import VAR\nfrom prophet import Prophet\nfrom sklearn.svm import SVR\n#import statsmodels.api as sm\n\n\n# Define the models\nmodels = {\n    'lstm': lambda input_shape: Sequential([ #different coding\n        LSTM(units=64, input_shape=(1, look_back)),\n        Dense(units=1)\n    ]),\n    'xgb': XGBRegressor(objective='reg:squarederror', n_estimators=25), \n    'rf': RandomForestRegressor(n_estimators=100),\n    #'var': VAR #different coding\n    'dt': DecisionTreeRegressor(max_depth = 20),\n    'prophet': Prophet, #different coding\n    'svr': SVR(kernel='rbf', gamma=0.5, C=10, epsilon = 0.05)\n}\n\n# model= models['xgb']\n\n\n\n# transform a time series dataset into a supervised learning dataset\n#creating a matrix where each row represens a time step and each column represents a feauture.\n#n_in (number of time steps to use as input for the model)\n#n_out (number of time steps to forecast)\ndef series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n\tn_vars = 1 if type(data) is list else data.shape[1]\n\tdf = DataFrame(data[:,1:])\n\ttimestamp=DataFrame(data[:,1])\n\tcols = list()\n\t# input sequence (t-n, ... t-1)\n\tfor i in range(n_in, 0, -1):\n\t\tcols.append(df.shift(i))\n\t# forecast sequence (t, t+1, ... t+n)\n\tfor i in range(0, n_out):\n\t\tcols.append(df.shift(-i))\n\t# put it all together\n\tagg = concat(cols, axis=1)\n\t# drop rows with NaN values\n\tif dropnan:\n\t\tagg.dropna(inplace=True)\n\treturn agg.values\n\n# split a univariate dataset into train/test sets\n#The input parameters are data (the dataset to split) and n_test (the number of time steps to use for testing)\ndef train_test_split(data, n_test):\n\treturn data[:-n_test, :], data[-n_test:, :]\n#The first numpy array returned, represents the training dataset, \n#which consists of all the observations in the original data array except for the last n_test observations.\n#The second numpy array returned, represents the test dataset,\n#which consists of the last n_test observations of the original data array.\n\n# fit an random forest model and make a one step prediction\ndef random_forest_forecast(model_name, train, testX, feautures, n_out):\n\t# transform list into array\n\ttrain = asarray(train)\n\t# split into input and output columns\n\ttrainX, trainy = train[:, :-1*feautures*n_out], train[:,-1*feautures*n_out:]\n\t#print('random_forest_shapes',trainX.shape,trainy.shape,trainy[0],train.shape)\n\t# fit model\n\t#model = RandomForestRegressor(n_estimators=1000) #1000 trees -- Change it and see!\n\t#model=DecisionTreeRegressor(max_depth = 3)    \n\tmodel= models[model_name]\n\t# LSTM Parameters\n\t#model = Sequential()\n\t#look_back=1\n\t#model.add(LSTM(4, input_shape=(1, look_back)))\n\t#model.add(Dense(1))\n\t#model.compile(loss='mean_squared_error', optimizer='adam')\n\t#model.set_default_exog_names(0)\n\tmodel.fit(trainX, trainy)\n\t# make a one-step prediction\n\tyhat = model.predict([testX])\n\t#print(yhat.shape,yhat)\n\treturn yhat[0]\n\n# walk-forward validation for univariate data\ndef walk_forward_validation(model_name, data, feautures, n_out, n_test):\n\t#print(data.shape,data[0])\n\tpredictions = list()\n\t# split dataset\n\ttrain, test = train_test_split(data, n_test)\n\t#print('shape',train.shape,test.shape)   \n\t# seed history with training dataset\n\thistory = [x for x in train]\n\t#print(history[0])\n\t# step over each time-step in the test set\n\tfor i in range(len(test)):\n\t\t# split test row into input and output columns\n\t\ttestX, testy = test[i, :-1*feautures*n_out], test[i, -1*feautures*n_out:]\n\t\t# fit model on history and make a prediction\n\t\tyhat = random_forest_forecast(model_name, history, testX, feautures, n_out)\n\t\t# store forecast in list of predictions\n\t\tpredictions.append(yhat)\n\t\t# add actual observation to history for the next loop\n\t\thistory.append(test[i])\n\t\toutput = \"(expected,predicted):\"\n\t\tfor j in range(feautures*n_out):\n\t\t\toutput = output  + \"(\" + '{0:.7f}'.format(test[i][j]) +\",\" + '{0:.7f}'.format(yhat[j]) + \")\"\n\t\tprint(output)\n\t\t#print(\"yhat\",i, \":\",yhat)\n\t\t# summarize progress\n\t\t#print('>expected=%.7f, predicted=%.7f, expected=%.6f, predicted=%.6f, expected=%.9f, predicted=%.9f' % (testy[0], yhat[0], testy[1], yhat[1], testy[2], yhat[2]))#change\n\t\t#print('inwalk',yhat)\n\t\t#print((testy, yhat))        \n\t# estimate prediction error\n\tp=np.array(predictions)\n\t# Mean Absolute Error\n\tmae = [mean_absolute_error(test[:,-1*feautures*n_out+j],p[:,j]) for j in range(feautures*n_out)]\n\t# Mean Squared Error\n\tmse = [mean_squared_error(test[:,-1*feautures*n_out+j],p[:,j]) for j in range(feautures*n_out)]\n\t# MAPE- Mean absolute percentage error\n\tmape = [mean_absolute_percentage_error(test[:,-1*feautures*n_out+j],p[:,j]) for j in range(feautures*n_out)]    \n\t# Median absolute error\n\tMedAE = [median_absolute_error(test[:,-1*feautures*n_out+j],p[:,j]) for j in range(feautures*n_out)]\n\t# Max error\n\tMaxE = [max_error(test[:,-1*feautures*n_out+j],p[:,j]) for j in range(feautures*n_out)]\n\t# R² score, the coefficient of determination\n\tr2 = [r2_score(test[:,-1*feautures*n_out+j],p[:,j]) for j in range(feautures*n_out)]\n\t# Explained variance score\n\tevs = [explained_variance_score(test[:,-1*feautures*n_out+j],p[:,j]) for j in range(feautures*n_out)] \n\t#error = [mean_absolute_error(test[:, -3], p[:,2]),mean_absolute_error(test[:, -2], p[:,0]),mean_absolute_error(test[:, -1], p[:,1])]\n\treturn [mae,mse,mape,MedAE,MaxE,r2,evs], test[:,-1*feautures*n_out:], predictions\n\ndef predict(model_name, values, n_in, n_out, n_test):\n\t# load the dataset\n\tvalues = ship_data.values\n\t#values = float_array.values # VAR --else raws.values\n\t#values =np.hstack(values, input_column)\n\t# transform the time series data into supervised learning\n\tdata = series_to_supervised(values, n_in, n_out)    \n\tfeautures =  values.shape[1]-1\n\t#model = sm.tsa.vector_ar.var_model.VAR(endog=data)\n\t#model.set_default_exog_names(0) # VAR set the exog_names attribute to zero\n\t# evaluate\n\treturn walk_forward_validation(model_name, data, feautures, n_out, n_test)\n\n\n# HOMEMADE DEF\n\nimport matplotlib.pyplot as plt\nfrom matplotlib.gridspec import GridSpec\n\ndef print_everything(model_name, values, n_in, n_out, n_test):\n    print(\"\\nThe name of the model is:\", model_name)\n    model= models[model_name]\n    print(\"\\nExpected vs Predicted:\")\n    error , y , yhat  = predict(model_name, values, n_in, n_out, n_test)\n    \n    print(\"\\nThe error values for\", model_name)\n    MAE = \", \".join([\"%.10f\" % member for member in error[0]])\n    MSE = \", \".join([\"%.10f\" % member for member in error[1]])\n    mape = \", \".join([\"%.10f\" % member for member in error[2]])\n    MedAE = \", \".join([\"%.10f\" % member for member in error[3]])\n    MaxE = \", \".join([\"%.10f\" % member for member in error[4]])\n    r2 = \", \".join([\"%.5f\" % member for member in error[5]]) # it doesnt really apply in this\n    evs = \", \".join([\"%.5f\" % member for member in error[6]])\n    print(f\"MAE = {MAE} \\nMSE = {MSE} \\nmape = {mape} \\nMedAE = {MedAE} \\nMaxE = {MaxE} \\nr2 = {r2} \\nevs = {evs}\")\n    print(\"\\n\\n\")\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2023-04-23T20:24:47.696395Z","iopub.execute_input":"2023-04-23T20:24:47.696808Z","iopub.status.idle":"2023-04-23T20:24:47.732209Z","shell.execute_reply.started":"2023-04-23T20:24:47.696770Z","shell.execute_reply":"2023-04-23T20:24:47.730554Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The error message suggests that the input shape of the model is incorrect. Specifically, the expected shape is (None, 1, 1) while the actual shape is (None, 30). This means that the model is expecting a 3D tensor with shape (batch_size, timesteps, features) but it is receiving a 2D tensor with shape (batch_size, features).\n\nOne possible solution is to reshape the input data to have the expected shape. You can do this using the reshape() function from NumPy. Here's an example:","metadata":{}},{"cell_type":"code","source":"\"\"\"\nprint(\"rf - 5 - 1 - 1000\")\nprint_everything('rf', ship_data, 5, 1, 1000)\nprint(\"xgb - 5 - 1 - 1000\")\nprint_everything('xgb', ship_data, 5, 1, 1000)\nprint(\"dt - 5 - 1 - 1000\")\nprint_everything('dt', ship_data, 5, 1, 1000)\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2023-04-23T20:32:16.057228Z","iopub.execute_input":"2023-04-23T20:32:16.058389Z","iopub.status.idle":"2023-04-23T20:32:20.496734Z","shell.execute_reply.started":"2023-04-23T20:32:16.058344Z","shell.execute_reply":"2023-04-23T20:32:20.494535Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#error , y , yhat  = predict(float_array, 10, 2, 10)  #(raws, 10, 2, 100)\n#error , y , yhat  = predict('rf', ship_data, 10, 1, 50)\n#print([\"%.2f\" % member for member in error[0]])","metadata":{"execution":{"iopub.status.busy":"2023-04-23T14:49:31.535327Z","iopub.execute_input":"2023-04-23T14:49:31.535812Z","iopub.status.idle":"2023-04-23T14:49:42.032670Z","shell.execute_reply.started":"2023-04-23T14:49:31.535771Z","shell.execute_reply":"2023-04-23T14:49:42.030906Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n#MAE = \", \".join([\"%.10f\" % member for member in error[0]])\n#MSE = \", \".join([\"%.10f\" % member for member in error[1]])\n#mape = \", \".join([\"%.10f\" % member for member in error[2]])\n#MedAE = \", \".join([\"%.10f\" % member for member in error[3]])\n#MaxE = \", \".join([\"%.10f\" % member for member in error[4]])\n#r2 = \", \".join([\"%.5f\" % member for member in error[5]]) # it doesnt really apply in this\n#evs = \", \".join([\"%.5f\" % member for member in error[6]])\n#print(f\"MAE = {MAE} \\nMSE = {MSE} \\nmape = {mape} \\nMedAE = {MedAE} \\nMaxE = {MaxE} \\nr2 = {r2} \\nevs = {evs}\")\n","metadata":{"execution":{"iopub.status.busy":"2023-04-23T19:01:06.900959Z","iopub.execute_input":"2023-04-23T19:01:06.901754Z","iopub.status.idle":"2023-04-23T19:01:06.908800Z","shell.execute_reply.started":"2023-04-23T19:01:06.901667Z","shell.execute_reply":"2023-04-23T19:01:06.906983Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\nimport matplotlib.pyplot as plt\nfrom matplotlib.gridspec import GridSpec\n\nfig = plt.figure(figsize=(10, 10))\n\n# define gridspec\ngs = GridSpec(nrows=3, ncols=2, height_ratios=[1, 1, 2])\ngs.update(wspace=0.4, hspace=0.4)\n\n# create the list of values\nmae = [\"%.10f\" % member for member in error[0]]\nMSE = [\"%.10f\" % member for member in error[1]]\nmape = [\"%.10f\" % member for member in error[2]]\nMedAE = [\"%.10f\" % member for member in error[3]]\nMaxE =  [\"%.10f\" % member for member in error[4]]\nr2 = [\"%.5f\" % member for member in error[5]]\nevs =[\"%.5f\" % member for member in error[6]]\n\n# convert the values to floats\nmae = [float(v) for v in mae]\nMSE = [float(v) for v in MSE]\nmape = [float(v) for v in mape]\nMedAE = [float(v) for v in MedAE]\nMaxE = [float(v) for v in MaxE]\nr2 = [float(v) for v in r2]\nevs = [float(v) for v in evs]\n\n# create the x-axis values (index positions)\nindices = list(range(len(mae)))\nindices = list(range(len(MSE)))\nindices = list(range(len(mape)))\nindices = list(range(len(MedAE)))\nindices = list(range(len(MaxE)))\nindices = list(range(len(r2)))\nindices = list(range(len(evs)))\n\nfig, axs = plt.subplots(nrows=2, ncols=4, figsize=(15, 6))\n\n# create the histogram\naxs[0,0].bar(indices, mae)\naxs[0,1].bar(indices, MSE)\naxs[0,2].bar(indices, mape)\naxs[0,3].bar(indices, MedAE)\naxs[1,0].bar(indices, MaxE)\naxs[1,1].bar(indices, r2)\naxs[1,2].bar(indices, evs)\n\n#axs[0].hist(mae, bins=10)\n#axs[1].hist(MSE, bins=10)\n#axs[2].hist(mape, bins=10)\n#axs[3].hist(MedAE, bins=10)\n#axs[4].hist(MaxE, bins=10)\n#axs[5].hist(r2, bins=10)\n#axs[6].hist(evs, bins=10)\n\n#axs[0,0] = fig.add_subplot(gs[0, 0])\naxs[0,0].set(xlabel='MAE Values', ylabel='Frequency', title='MAE')\n#axs[1] = fig.add_subplot(gs[0, 1])\naxs[0,1].set(xlabel='MSE Values', ylabel='Frequency', title='MSE')\n#axs[2] = fig.add_subplot(gs[1, 0])\naxs[0,2].set(xlabel='Mape Values', ylabel='Frequency', title='Mape')\n#axs[3] = fig.add_subplot(gs[1, 1])\naxs[0,3].set(xlabel='MedAE Values', ylabel='Frequency', title='MedAE')\n#axs[4] = fig.add_subplot(gs[2, :])\naxs[1,0].set(xlabel='MaxE Values', ylabel='Frequency', title='MaxE')\naxs[1,1].set(xlabel='R2 Values', ylabel='Frequency', title='R2')\naxs[1,2].set(xlabel='EVS Values', ylabel='Frequency', title='EVS')\n\nfig.subplots_adjust(left=0.06, bottom=0.1, right=0.95, top=0.95, wspace=0.6, hspace=0.5)\n\nplt.show()\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2023-04-23T19:01:49.278048Z","iopub.execute_input":"2023-04-23T19:01:49.279397Z","iopub.status.idle":"2023-04-23T19:01:49.289575Z","shell.execute_reply.started":"2023-04-23T19:01:49.279320Z","shell.execute_reply":"2023-04-23T19:01:49.288133Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\nfig, axs = plt.subplots(2, 3, figsize=(15, 8))\n\n# subplot 1\np=np.array(yhat)\naxs[0, 0].plot(y[:,0], label='Expected Long')\naxs[0, 0].plot(p[:,0], label='Predicted Long')\naxs[0, 0].legend()\n\n# subplot 2\naxs[0, 1].plot(y[:,1], label='Expected Lat')\naxs[0, 1].plot(p[:,1], label='Predicted Lat')\naxs[0, 1].legend()\n\n# subplot 3\naxs[0, 2].plot(y[:,2], label='Expected Speed')\naxs[0, 2].plot(p[:,2], label='Predicted Speed')\naxs[0, 2].legend()\n\n# subplot 4\naxs[1, 0].plot(y[:,3], label='Expected Long')\naxs[1, 0].plot(p[:,3], label='Predicted Long')\naxs[1, 0].legend()\n\n# subplot 5\naxs[1, 1].plot(y[:,4], label='Expected Lat')\naxs[1, 1].plot(p[:,4], label='Predicted Lat')\naxs[1, 1].legend()\n\n# subplot 6\naxs[1, 2].plot(y[:,5], label='Expected Speed')\naxs[1, 2].plot(p[:,5], label='Predicted Speed')\naxs[1, 2].legend()\n\n# plot\nplt.show()\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2023-04-23T13:49:12.901109Z","iopub.execute_input":"2023-04-23T13:49:12.901566Z","iopub.status.idle":"2023-04-23T13:49:13.841046Z","shell.execute_reply.started":"2023-04-23T13:49:12.901516Z","shell.execute_reply":"2023-04-23T13:49:13.839846Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# SVR","metadata":{}},{"cell_type":"code","source":"\"\"\"\nimport numpy as np\nfrom numpy import asarray\nfrom pandas import DataFrame\nfrom pandas import concat\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_absolute_percentage_error\nfrom sklearn.metrics import median_absolute_error\nfrom sklearn.metrics import max_error\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import explained_variance_score\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom matplotlib import pyplot\nfrom xgboost import XGBRegressor\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.layers import LSTM\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import StandardScaler\n#from statsmodels.tsa.arima.model import ARIMA\nfrom statsmodels.tsa.vector_ar.var_model import VAR\nfrom prophet import Prophet\nfrom sklearn.svm import SVR\n#import statsmodels.api as sm\n\n\n# Define the models\nmodels = {\n    'lstm': lambda input_shape: Sequential([ #different coding\n        LSTM(units=64, input_shape=(1, look_back)),\n        Dense(units=1)\n    ]),\n    'xgb': XGBRegressor(objective='reg:squarederror', n_estimators=25), \n    'rf': RandomForestRegressor(n_estimators=100),\n    #'var': VAR #different coding\n    'dt': DecisionTreeRegressor(max_depth = 20),\n    'prophet': Prophet, #different coding\n    'svr': SVR(kernel='rbf', gamma=0.5, C=10, epsilon = 0.05)\n}\n\n# model= models['xgb']\n\n\n\n# transform a time series dataset into a supervised learning dataset\n#creating a matrix where each row represens a time step and each column represents a feauture.\n#n_in (number of time steps to use as input for the model)\n#n_out (number of time steps to forecast)\ndef series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n\tn_vars = 1 if type(data) is list else data.shape[1]\n\tdf = DataFrame(data[:,1:])\n\ttimestamp=DataFrame(data[:,1])\n\tcols = list()\n\t# input sequence (t-n, ... t-1)\n\tfor i in range(n_in, 0, -1):\n\t\tcols.append(df.shift(i))\n\t# forecast sequence (t, t+1, ... t+n)\n\tfor i in range(0, n_out):\n\t\tcols.append(df.shift(-i))\n\t# put it all together\n\tagg = concat(cols, axis=1)\n\t# drop rows with NaN values\n\tif dropnan:\n\t\tagg.dropna(inplace=True)\n\treturn agg.values\n\n# split a univariate dataset into train/test sets\n#The input parameters are data (the dataset to split) and n_test (the number of time steps to use for testing)\ndef train_test_split(data, n_test):\n\treturn data[:-n_test, :], data[-n_test:, :]\n#The first numpy array returned, represents the training dataset, \n#which consists of all the observations in the original data array except for the last n_test observations.\n#The second numpy array returned, represents the test dataset,\n#which consists of the last n_test observations of the original data array.\n\n# fit an random forest model and make a one step prediction\ndef random_forest_forecast(model_name, train, testX, feautures, n_out):\n\t# transform list into array\n\ttrain = asarray(train)\n\t# split into input and output columns\n\ttrainX, trainy = train[:, :-1*feautures*n_out], train[:,-1*feautures*n_out:]\n\t#print('random_forest_shapes',trainX.shape,trainy.shape,trainy[0],train.shape)\n\t# fit model\n\t#model = RandomForestRegressor(n_estimators=1000) #1000 trees -- Change it and see!\n\t#model=DecisionTreeRegressor(max_depth = 3)    \n\tmodel= models[model_name]\n\t# LSTM Parameters\n\t#model = Sequential()\n\t#look_back=1\n\t#model.add(LSTM(4, input_shape=(1, look_back)))\n\t#model.add(Dense(1))\n\t#model.compile(loss='mean_squared_error', optimizer='adam')\n\tsc = StandardScaler()\n\ttrainX = sc.fit_transform(X_train)\n\tX_test = sc.transform(X_test)\n\n\tscaler = MinMaxScaler()\n\ttrainX = scaler.fit_transform(trainX)\n\ttrainy = scaler.fit_transform(trainy)   \n\t#model.set_default_exog_names(0)\n\tmodel.fit(trainX, trainy)\n\t# make a one-step prediction\n\tyhat = model.predict([testX])\n\t#print(yhat.shape,yhat)\n\treturn yhat[0]\n\n# walk-forward validation for univariate data\ndef walk_forward_validation(model_name, data, feautures, n_out, n_test):\n\t#print(data.shape,data[0])\n\tpredictions = list()\n\t# split dataset\n\ttrain, test = train_test_split(data, n_test)\n\t#print('shape',train.shape,test.shape)   \n\t# seed history with training dataset\n\t#history = [x for x in train]\n\t#print(history[0])\n\t# step over each time-step in the test set\n\tfor i in range(len(test)):\n\t\t# split test row into input and output columns\n\t\ttestX, testy = test[i, :-1*feautures*n_out], test[i, -1*feautures*n_out:]\n\t\t# fit model on history and make a prediction\n\t\tyhat = random_forest_forecast(model_name,train,testX, feautures, n_out) #history,\n\t\t# store forecast in list of predictions\n\t\tpredictions.append(yhat)\n\t\t# add actual observation to history for the next loop\n\t\t#history.append(test[i])\n\t\toutput = \"(expected,predicted):\"\n\t\tfor j in range(feautures*n_out):\n\t\t\toutput = output  + \"(\" + '{0:.7f}'.format(test[i][j]) +\",\" + '{0:.7f}'.format(yhat[j]) + \")\"\n\t\tprint(output)\n\t\t#print(\"yhat\",i, \":\",yhat)\n\t\t# summarize progress\n\t\t#print('>expected=%.7f, predicted=%.7f, expected=%.6f, predicted=%.6f, expected=%.9f, predicted=%.9f' % (testy[0], yhat[0], testy[1], yhat[1], testy[2], yhat[2]))#change\n\t\t#print('inwalk',yhat)\n\t\t#print((testy, yhat))        \n\t# estimate prediction error\n\tp=np.array(predictions)\n\t# Mean Absolute Error\n\tmae = [mean_absolute_error(test[:,-1*feautures*n_out+j],p[:,j]) for j in range(feautures*n_out)]\n\t# Mean Squared Error\n\tmse = [mean_squared_error(test[:,-1*feautures*n_out+j],p[:,j]) for j in range(feautures*n_out)]\n\t# MAPE- Mean absolute percentage error\n\tmape = [mean_absolute_percentage_error(test[:,-1*feautures*n_out+j],p[:,j]) for j in range(feautures*n_out)]    \n\t# Median absolute error\n\tMedAE = [median_absolute_error(test[:,-1*feautures*n_out+j],p[:,j]) for j in range(feautures*n_out)]\n\t# Max error\n\tMaxE = [max_error(test[:,-1*feautures*n_out+j],p[:,j]) for j in range(feautures*n_out)]\n\t# R² score, the coefficient of determination\n\tr2 = [r2_score(test[:,-1*feautures*n_out+j],p[:,j]) for j in range(feautures*n_out)]\n\t# Explained variance score\n\tevs = [explained_variance_score(test[:,-1*feautures*n_out+j],p[:,j]) for j in range(feautures*n_out)] \n\t#error = [mean_absolute_error(test[:, -3], p[:,2]),mean_absolute_error(test[:, -2], p[:,0]),mean_absolute_error(test[:, -1], p[:,1])]\n\treturn [mae,mse,mape,MedAE,MaxE,r2,evs], test[:,-1*feautures*n_out:], predictions\n\ndef predict(model_name, values, n_in, n_out, n_test):\n\t# load the dataset\n\tvalues = ship_data.values\n\t#values = float_array.values # VAR --else raws.values\n\t#values =np.hstack(values, input_column)\n\t# transform the time series data into supervised learning\n\tdata = series_to_supervised(values, n_in, n_out)    \n\tfeautures =  values.shape[1]-1\n\t#model = sm.tsa.vector_ar.var_model.VAR(endog=data)\n\t#model.set_default_exog_names(0) # VAR set the exog_names attribute to zero\n\t# evaluate\n\treturn walk_forward_validation(model_name, data, feautures, n_out, n_test)\n\n\n# HOMEMADE DEF\n\nimport matplotlib.pyplot as plt\nfrom matplotlib.gridspec import GridSpec\n\ndef print_everything(model_name, values, n_in, n_out, n_test):\n    print(\"\\nThe name of the model is:\", model_name)\n    model= models[model_name]\n    print(\"\\nExpected vs Predicted:\")\n    error , y , yhat  = predict(model_name, values, n_in, n_out, n_test)\n    \n    print(\"\\nThe error values for\", model_name)\n    MAE = \", \".join([\"%.10f\" % member for member in error[0]])\n    MSE = \", \".join([\"%.10f\" % member for member in error[1]])\n    mape = \", \".join([\"%.10f\" % member for member in error[2]])\n    MedAE = \", \".join([\"%.10f\" % member for member in error[3]])\n    MaxE = \", \".join([\"%.10f\" % member for member in error[4]])\n    r2 = \", \".join([\"%.5f\" % member for member in error[5]]) # it doesnt really apply in this\n    evs = \", \".join([\"%.5f\" % member for member in error[6]])\n    print(f\"MAE = {MAE} \\nMSE = {MSE} \\nmape = {mape} \\nMedAE = {MedAE} \\nMaxE = {MaxE} \\nr2 = {r2} \\nevs = {evs}\")\n    print(\"\\n\\n\")\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2023-04-23T19:53:01.760442Z","iopub.execute_input":"2023-04-23T19:53:01.760872Z","iopub.status.idle":"2023-04-23T19:53:01.798015Z","shell.execute_reply.started":"2023-04-23T19:53:01.760820Z","shell.execute_reply":"2023-04-23T19:53:01.796324Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#print(\"svr - 10 - 1 - 10\")\n#print_everything('svr', ship_data, 10, 1, 10)","metadata":{"execution":{"iopub.status.busy":"2023-04-23T20:07:04.352241Z","iopub.execute_input":"2023-04-23T20:07:04.353024Z","iopub.status.idle":"2023-04-23T20:07:04.359088Z","shell.execute_reply.started":"2023-04-23T20:07:04.352974Z","shell.execute_reply":"2023-04-23T20:07:04.357657Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\nimport numpy as np\nfrom numpy import asarray\nfrom pandas import DataFrame\nfrom pandas import concat\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_absolute_percentage_error\nfrom sklearn.metrics import median_absolute_error\nfrom sklearn.metrics import max_error\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import explained_variance_score\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom matplotlib import pyplot\nfrom xgboost import XGBRegressor\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.layers import LSTM\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import MinMaxScaler\n#from statsmodels.tsa.arima.model import ARIMA\nfrom statsmodels.tsa.vector_ar.var_model import VAR\nfrom prophet import Prophet\nfrom sklearn.svm import SVR\n#import statsmodels.api as sm\nimport datetime\n\n\n###############################################################################################################\n# Load the CSV file into a pandas dataframe\ndf = pd.read_csv(\"/kaggle/input/aisbrest/ais_brest_locations.csv\")\n\n# Fetch all rows where the ship ID is 227362110\nship_data = df[df['id'] == 227362110]\n\n\n# Select only the desired columns and sort by timestamp\ndesired_columns = ['timestamp','longitude', 'latitude','speed']\nship_data = ship_data[desired_columns].sort_values(by='timestamp')\n\n#print(ship_data)\n#print(ship_data.timestamp)\n#print(ship_data.timestamp.tail(50))\n\nprint(ship_data.shape)\ntest_len = 100\ntrain_len = 1908\n\ntrain = ship_data.iloc[:train_len, :][['longitude', 'latitude', 'speed']]\ntest = ship_data.iloc[-test_len:, :][['longitude', 'latitude', 'speed']]\n\n\nprint('Training data shape: ', train.shape)\nprint('Test data shape: ', test.shape)\n\n#print(train)\n#print(test)\n\n###############################################################################################################\nscaler = MinMaxScaler()\ntrain = scaler.fit_transform(train)\n\ntest = scaler.transform(test)\n\n# Converting to numpy arrays\ntrain_data = train.values\ntest_data = test.values\n\ntimesteps=5\n\ntrain_data_timesteps=np.array([[j for j in train_data[i:i+timesteps]] for i in range(0,len(train_data)-timesteps+1)])[:,:,0]\ntrain_data_timesteps.shape\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2023-04-22T15:43:16.621078Z","iopub.execute_input":"2023-04-22T15:43:16.621589Z","iopub.status.idle":"2023-04-22T15:43:18.077855Z","shell.execute_reply.started":"2023-04-22T15:43:16.621546Z","shell.execute_reply":"2023-04-22T15:43:18.076297Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# LSTM -Multiple Parallel Input and Multi-Step Output","metadata":{}},{"cell_type":"code","source":"\"\"\"\n# multivariate output stacked lstm example\nfrom numpy import array\nfrom numpy import hstack\nfrom keras.models import Sequential\nfrom keras.layers import LSTM\nfrom keras.layers import Dense\n\n# split a multivariate sequence into samples\ndef split_sequences(sequences, n_steps):\n\tX, y = list(), list()\n\tfor i in range(len(sequences)):\n\t\t# find the end of this pattern\n\t\tend_ix = i + n_steps\n\t\t# check if we are beyond the dataset\n\t\tif end_ix > len(sequences)-1:\n\t\t\tbreak\n\t\t# gather input and output parts of the pattern\n\t\tseq_x, seq_y = sequences[i:end_ix, :], sequences[end_ix, :]\n\t\tX.append(seq_x)\n\t\ty.append(seq_y)\n\treturn array(X), array(y)\n\n# define input sequence\n#in_seq1 = array([10, 20, 30, 40, 50, 60, 70, 80, 90])\n#in_seq2 = array([15, 25, 35, 45, 55, 65, 75, 85, 95])\nin_seq1 = array(ship_data.longitude)\nin_seq2 = array(ship_data.latitude)\nin_seq3 = array(ship_data.speed)\nout_seq = array([in_seq1[i]+in_seq2[i] for i in range(len(in_seq1))])\n# convert to [rows, columns] structure\nin_seq1 = in_seq1.reshape((len(in_seq1), 1))\nin_seq2 = in_seq2.reshape((len(in_seq2), 1))\nin_seq3 = in_seq3.reshape((len(in_seq3), 1))\nout_seq = out_seq.reshape((len(out_seq), 1))\n# horizontally stack columns\ndataset = hstack((in_seq1, in_seq2,in_seq3, out_seq))\n# choose a number of time steps\nn_steps = 3\n# convert into input/output\nX, y = split_sequences(dataset, n_steps)\n# the dataset knows the number of features, e.g. 2\nn_features = X.shape[2]\n# define model\nmodel = Sequential()\nmodel.add(LSTM(100, activation='relu', return_sequences=True, input_shape=(n_steps, n_features)))\nmodel.add(LSTM(100, activation='relu'))\nmodel.add(Dense(n_features))\nmodel.compile(optimizer='adam', loss='mse')\n# fit model\nmodel.fit(X, y, epochs=50, verbose=0)\n# demonstrate prediction\n#x_input = array([[70,75,145], [80,85,165], [90,95,185]])\n#x_input = x_input.reshape((1, n_steps, n_features))\n#yhat = model.predict(x_input, verbose=0)\n#print(yhat)\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2023-04-22T16:41:45.017592Z","iopub.execute_input":"2023-04-22T16:41:45.018001Z","iopub.status.idle":"2023-04-22T16:42:09.768331Z","shell.execute_reply.started":"2023-04-22T16:41:45.017959Z","shell.execute_reply":"2023-04-22T16:42:09.767029Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}}]}